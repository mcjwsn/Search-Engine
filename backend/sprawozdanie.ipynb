{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cc47d6",
   "metadata": {},
   "source": [
    "# Data set\n",
    "\n",
    "Do przechowywania danych użyto SQLite, ze względu na jego komaptybilność z Rsutem i Reactem( w których projekt zostało napisany), prostą implementację, a szczególnie na szybki i nisko-kosztowny dostęp do danych. Jeśli miałby to być serwer z większą ilością zapytań to wybrałbym Mongo, ale tutaj dla offline, prostej i nie obsługującej dużej liczby zapytań jest baza SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb9e15",
   "metadata": {},
   "source": [
    "Dane pobrano ze strony: https://dumps.wikimedia.org/enwiki/latest/ \n",
    "\n",
    "\n",
    "Posłużono się wersją Simple Wiki [Eng]\n",
    "\n",
    "Dane z pliku xml przeparsowano na pliki txt wikiextracotrem(https://github.com/attardi/wikiextractor) w lekko zmodyfikowanej wersji dla tego zadania\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6b2fd",
   "metadata": {},
   "source": [
    "Oddzielnie wprowadzono dane z plików txt do bazy sqlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa06de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def parse_file(file_path):\n",
    "    docs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        pattern = r'<doc id=\"(\\d+)\" url=\"(https?://[^\"]+)\" title=\"([^\"]+)\">([^<]+)<\\/doc>'\n",
    "        matches = re.findall(pattern, content)\n",
    "        for match in matches:\n",
    "            doc_id = match[0]\n",
    "            url = match[1]\n",
    "            title = match[2]\n",
    "            text = match[3]\n",
    "            docs.append((doc_id, title, url, text))\n",
    "    return docs\n",
    "\n",
    "def create_db_and_insert_data(docs):\n",
    "    conn = sqlite3.connect('articles.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            url TEXT,\n",
    "            text TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    cursor.executemany('''\n",
    "        INSERT INTO articles (id, title, url, text)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', docs)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    folders = ['AA/*', 'AC/*', 'AB/*']\n",
    "    for file in folders:\n",
    "        file_paths = glob.glob(f)\n",
    "        all_docs = []\n",
    "        for file_path in file_paths:\n",
    "            print(f'Przetwarzam plik: {file_path}')\n",
    "            docs = parse_file(file_path)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        if all_docs:\n",
    "            create_db_and_insert_data(all_docs)\n",
    "            print(\"Dane zostały zapisane w bazie danych.\")\n",
    "        else:\n",
    "            print(\"Brak danych do zapisania.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf38c7",
   "metadata": {},
   "source": [
    "Alternatywna wersja rozwiazania( uzywanie api wikipedii i sciagnie losowych artykulow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacja wspomagana przez AI\n",
    "import sqlite3\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATABASE_NAME = \"wikipedia_fast.db\"\n",
    "WIKIPEDIA_API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "TARGET_ARTICLE_COUNT = 300000\n",
    "THREADS = 50  \n",
    "BATCH_SIZE = 500 \n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT UNIQUE,\n",
    "        content TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_title ON articles(title)\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_article_list(limit):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": limit,\n",
    "        \"apfilterredir\": \"nonredirects\"\n",
    "    }\n",
    "    response = requests.get(WIKIPEDIA_API_URL, params=params)\n",
    "    data = response.json()\n",
    "    return [page[\"title\"] for page in data[\"query\"][\"allpages\"]]\n",
    "\n",
    "def fetch_article(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"titles\": title,\n",
    "            \"explaintext\": True,\n",
    "            \"exsectionformat\": \"plain\"\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        return (title, page.get(\"extract\", \"\"))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def save_batch(batch):\n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(\n",
    "        \"INSERT OR IGNORE INTO articles (title, content) VALUES (?, ?)\",\n",
    "        [(title, content) for title, content in batch if content]\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    init_db()\n",
    "    \n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM articles\")\n",
    "    existing = cursor.fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    if existing >= TARGET_ARTICLE_COUNT: return\n",
    "    \n",
    "    needed = TARGET_ARTICLE_COUNT - existing\n",
    "    \n",
    "    print(\"Pobieranie...\")\n",
    "    articles = get_article_list(needed * 2)\n",
    "    articles = articles[:needed + 10000] \n",
    "    \n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        futures = {executor.submit(fetch_article, title): title for title in articles}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Pob\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                batch.append(result)\n",
    "                \n",
    "                if len(batch) >= BATCH_SIZE:\n",
    "                    save_batch(batch)\n",
    "                    batch = []\n",
    "    \n",
    "    if batch:\n",
    "        save_batch(batch)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Zakonczono w {elapsed:.2f} seknd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1b8ca",
   "metadata": {},
   "source": [
    "Prosty Web scraper( nie uzyty, bo dziala za wolno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "NUM_ARTICLES = 300000\n",
    "\n",
    "conn = sqlite3.connect('articles.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        url TEXT UNIQUE,\n",
    "        title TEXT,\n",
    "        text TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "def scrape_random_article():\n",
    "    try:\n",
    "        response = requests.get('https://pl.wikipedia.org/wiki/Special:Random', allow_redirects=True)\n",
    "        final_url = response.url \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1').get_text()\n",
    "        paragraphs = soup.select('div.mw-parser-output > p')\n",
    "        text = '\\n'.join(p.get_text() for p in paragraphs if p.get_text(strip=True))\n",
    "        return final_url, title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "for _ in range(NUM_ARTICLES):\n",
    "    url, title, text = scrape_random_article()\n",
    "    if title and text:\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                'INSERT INTO articles (url, title, text) VALUES (?, ?, ?)',\n",
    "                (url, title, text)\n",
    "            )\n",
    "            conn.commit()\n",
    "        except sqlite3.IntegrityError:\n",
    "    time.sleep(1)  # zeby nie zablokowac serwera\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e27e17",
   "metadata": {},
   "source": [
    "Łącznie użyto ~350k dokumentów ( 370847), a słownik zawierał ~500k słów (494618)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a096ff",
   "metadata": {},
   "source": [
    "# Frontend\n",
    "Implementacja w React\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050351e",
   "metadata": {},
   "source": [
    "# Backend\n",
    "\n",
    "Realizowany w Ruscie\n",
    "\n",
    "Do komunikacji Actix(api do rusta) na localhoscie"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
