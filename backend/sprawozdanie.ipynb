{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cc47d6",
   "metadata": {},
   "source": [
    "# Data set\n",
    "\n",
    "Do przechowywania danych użyto SQLite, ze względu na jego komaptybilność z Rsutem i Reactem( w których projekt zostało napisany), prostą implementację, a szczególnie na szybki i nisko-kosztowny dostęp do danych. Jeśli miałby to być serwer z większą ilością zapytań to wybrałbym Mongo, ale tutaj dla offline, prostej i nie obsługującej dużej liczby zapytań jest baza SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb9e15",
   "metadata": {},
   "source": [
    "Dane pobrano ze strony: https://dumps.wikimedia.org/enwiki/latest/ \n",
    "\n",
    "\n",
    "Posłużono się wersją Simple Wiki [Eng]\n",
    "\n",
    "Dane z pliku xml przeparsowano na pliki txt wikiextracotrem(https://github.com/attardi/wikiextractor) w lekko zmodyfikowanej wersji dla tego zadania\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6b2fd",
   "metadata": {},
   "source": [
    "Oddzielnie wprowadzono dane z plików txt do bazy sqlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa06de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def parse_file(file_path):\n",
    "    docs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        pattern = r'<doc id=\"(\\d+)\" url=\"(https?://[^\"]+)\" title=\"([^\"]+)\">([^<]+)<\\/doc>'\n",
    "        matches = re.findall(pattern, content)\n",
    "        for match in matches:\n",
    "            doc_id = match[0]\n",
    "            url = match[1]\n",
    "            title = match[2]\n",
    "            text = match[3]\n",
    "            docs.append((doc_id, title, url, text))\n",
    "    return docs\n",
    "\n",
    "def create_db_and_insert_data(docs):\n",
    "    conn = sqlite3.connect('articles.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            url TEXT,\n",
    "            text TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    cursor.executemany('''\n",
    "        INSERT INTO articles (id, title, url, text)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', docs)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    folders = ['AA/*', 'AC/*', 'AB/*']\n",
    "    for file in folders:\n",
    "        file_paths = glob.glob(f)\n",
    "        all_docs = []\n",
    "        for file_path in file_paths:\n",
    "            print(f'Przetwarzam plik: {file_path}')\n",
    "            docs = parse_file(file_path)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        if all_docs:\n",
    "            create_db_and_insert_data(all_docs)\n",
    "            print(\"Dane zostały zapisane w bazie danych.\")\n",
    "        else:\n",
    "            print(\"Brak danych do zapisania.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf38c7",
   "metadata": {},
   "source": [
    "Alternatywna wersja rozwiazania( uzywanie api wikipedii i sciagnie losowych artykulow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacja wspomagana przez AI\n",
    "import sqlite3\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATABASE_NAME = \"wikipedia_fast.db\"\n",
    "WIKIPEDIA_API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "TARGET_ARTICLE_COUNT = 300000\n",
    "THREADS = 50  \n",
    "BATCH_SIZE = 500 \n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT UNIQUE,\n",
    "        content TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_title ON articles(title)\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_article_list(limit):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": limit,\n",
    "        \"apfilterredir\": \"nonredirects\"\n",
    "    }\n",
    "    response = requests.get(WIKIPEDIA_API_URL, params=params)\n",
    "    data = response.json()\n",
    "    return [page[\"title\"] for page in data[\"query\"][\"allpages\"]]\n",
    "\n",
    "def fetch_article(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"titles\": title,\n",
    "            \"explaintext\": True,\n",
    "            \"exsectionformat\": \"plain\"\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        return (title, page.get(\"extract\", \"\"))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def save_batch(batch):\n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(\n",
    "        \"INSERT OR IGNORE INTO articles (title, content) VALUES (?, ?)\",\n",
    "        [(title, content) for title, content in batch if content]\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    init_db()\n",
    "    \n",
    "    conn = sqlite3.connect(DATABASE_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM articles\")\n",
    "    existing = cursor.fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    if existing >= TARGET_ARTICLE_COUNT: return\n",
    "    \n",
    "    needed = TARGET_ARTICLE_COUNT - existing\n",
    "    \n",
    "    print(\"Pobieranie...\")\n",
    "    articles = get_article_list(needed * 2)\n",
    "    articles = articles[:needed + 10000] \n",
    "    \n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        futures = {executor.submit(fetch_article, title): title for title in articles}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Pob\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                batch.append(result)\n",
    "                \n",
    "                if len(batch) >= BATCH_SIZE:\n",
    "                    save_batch(batch)\n",
    "                    batch = []\n",
    "    \n",
    "    if batch:\n",
    "        save_batch(batch)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Zakonczono w {elapsed:.2f} seknd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1b8ca",
   "metadata": {},
   "source": [
    "Prosty Web scraper( nie uzyty, bo dziala za wolno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "NUM_ARTICLES = 300000\n",
    "\n",
    "conn = sqlite3.connect('articles.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        url TEXT UNIQUE,\n",
    "        title TEXT,\n",
    "        text TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "def scrape_random_article():\n",
    "    try:\n",
    "        response = requests.get('https://pl.wikipedia.org/wiki/Special:Random', allow_redirects=True)\n",
    "        final_url = response.url \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1').get_text()\n",
    "        paragraphs = soup.select('div.mw-parser-output > p')\n",
    "        text = '\\n'.join(p.get_text() for p in paragraphs if p.get_text(strip=True))\n",
    "        return final_url, title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "for _ in range(NUM_ARTICLES):\n",
    "    url, title, text = scrape_random_article()\n",
    "    if title and text:\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                'INSERT INTO articles (url, title, text) VALUES (?, ?, ?)',\n",
    "                (url, title, text)\n",
    "            )\n",
    "            conn.commit()\n",
    "        except sqlite3.IntegrityError:\n",
    "    time.sleep(1)  # zeby nie zablokowac serwera\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e27e17",
   "metadata": {},
   "source": [
    "Łącznie użyto ~350k dokumentów ( 370847), a słownik zawierał ~500k słów (494618)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a096ff",
   "metadata": {},
   "source": [
    "# Frontend\n",
    "Implementacja w React, kod i szczegóły dependecies itd są na githubie\n",
    "\n",
    "W Reacie jest liczony czas trwania obliczen( odpowiedzi od API), bo uznałem że w tym miejscu będzie bardziej odzwierciedlał rzeczywisty czas oczekiwania użytkownika na odpowiedź podczas wyszukiwania\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050351e",
   "metadata": {},
   "source": [
    "# Backend\n",
    "\n",
    "Realizowany w Ruscie\n",
    "\n",
    "Do komunikacji z frontem użyto Actix\n",
    "\n",
    "Uzyto Rust ze wzgledu na jego niskopoziomowa szybkosc i bezpieczne i szybkie zarzadznie pamiecią, kluczowe w tym zagadnieniu. Rust ma swoje minusy( brak implementacji użytecznych bibliotek) i plusy(sprawna wielowątkowość), co przy prędkość porównywalnej do C/C++ czyni go dobrym narzędziem do tego typu problemów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a06fd",
   "metadata": {},
   "source": [
    "Jeszcze przed rozpoczęciem obliczeń stowrzono plik stop_words, który wyelimunej nam nieużyteczne słowa.\n",
    "\n",
    "Dodatkowo zaimplementowano Porter Stemming Algorithm do przekształcenia wspólnych końcówek morfologicznych i fleksyjnych"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76f0581c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "fn is_vowel(word: &[char], i: usize) -> bool {\n",
    "    let c = word[i];\n",
    "    match c {\n",
    "        'a' | 'e' | 'i' | 'o' | 'u' => true,\n",
    "        'y' => i > 0 && !is_vowel(word, i - 1),\n",
    "        _ => false,\n",
    "    }\n",
    "}\n",
    "\n",
    "fn measure(word: &[char]) -> usize {\n",
    "    let mut m = 0;\n",
    "    let mut prev_vowel = false;\n",
    "\n",
    "    for i in 0..word.len() {\n",
    "        let current_vowel = is_vowel(word, i);\n",
    "        if prev_vowel && !current_vowel {\n",
    "            m += 1;\n",
    "        }\n",
    "        prev_vowel = current_vowel;\n",
    "    }\n",
    "\n",
    "    m\n",
    "}\n",
    "\n",
    "fn has_vowel(word: &[char]) -> bool {\n",
    "    word.iter().enumerate().any(|(i, _)| is_vowel(word, i))\n",
    "}\n",
    "\n",
    "fn ends_with_cvc(word: &[char]) -> bool {\n",
    "    if word.len() < 3 {\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    let i = word.len() - 3;\n",
    "    let last_c = word[i + 2];\n",
    "    !is_vowel(word, i) && is_vowel(word, i + 1) && !is_vowel(word, i + 2) &&\n",
    "        !['w', 'x', 'y'].contains(&last_c)\n",
    "}\n",
    "\n",
    "fn replace_suffix(word: &mut Vec<char>, suffix: &str, replacement: &str) -> bool {\n",
    "    let suffix_chars: Vec<char> = suffix.chars().collect();\n",
    "    let replacement_chars: Vec<char> = replacement.chars().collect();\n",
    "\n",
    "    if word.ends_with(&suffix_chars) {\n",
    "        let new_len = word.len() - suffix_chars.len();\n",
    "        word.truncate(new_len);\n",
    "        word.extend(replacement_chars);\n",
    "        true\n",
    "    } else {\n",
    "        false\n",
    "    }\n",
    "}\n",
    "\n",
    "fn replace_suffix_condition<F>(word: &mut Vec<char>, suffix: &str, replacement: &str, condition: F) -> bool\n",
    "where\n",
    "    F: Fn(&[char]) -> bool,\n",
    "{\n",
    "    let suffix_chars: Vec<char> = suffix.chars().collect();\n",
    "    if word.ends_with(&suffix_chars) {\n",
    "        let stem = &word[..word.len() - suffix_chars.len()];\n",
    "        if condition(stem) {\n",
    "            replace_suffix(word, suffix, replacement);\n",
    "            true\n",
    "        } else {\n",
    "            false\n",
    "        }\n",
    "    } else {\n",
    "        false\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_1a(word: &mut Vec<char>) {\n",
    "    if replace_suffix(word, \"sses\", \"ss\") { return }\n",
    "    if replace_suffix(word, \"ies\", \"i\") { return }\n",
    "    if replace_suffix(word, \"ss\", \"ss\") { return }\n",
    "\n",
    "    if word.ends_with(&['s']) {\n",
    "        let stem = &word[..word.len() - 1];\n",
    "        if has_vowel(stem) {\n",
    "            word.pop();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_1b(word: &mut Vec<char>) {\n",
    "    if replace_suffix_condition(word, \"eed\", \"ee\", |stem| measure(stem) > 0) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    let mut modified = false;\n",
    "    let original = word.clone();\n",
    "\n",
    "    if replace_suffix(word, \"ed\", \"\") && has_vowel(word) {\n",
    "        modified = true;\n",
    "    } else {\n",
    "        *word = original.clone();\n",
    "    }\n",
    "\n",
    "    if !modified && replace_suffix(word, \"ing\", \"\") && has_vowel(word) {\n",
    "        modified = true;\n",
    "    } else if !modified {\n",
    "        *word = original;\n",
    "    }\n",
    "\n",
    "    if modified {\n",
    "        if replace_suffix(word, \"at\", \"ate\") ||\n",
    "            replace_suffix(word, \"bl\", \"ble\") ||\n",
    "            replace_suffix(word, \"iz\", \"ize\") {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        if word.len() >= 2 {\n",
    "            let last = word[word.len() - 1];\n",
    "            let prev = word[word.len() - 2];\n",
    "            if last == prev && !is_vowel(word, word.len() - 1) && !['l', 's', 'z'].contains(&last) {\n",
    "                word.pop();\n",
    "                return;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if measure(word) == 1 && ends_with_cvc(word) {\n",
    "            word.push('e');\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_1c(word: &mut Vec<char>) {\n",
    "    if word.ends_with(&['y']) && has_vowel(&word[..word.len() - 1]) {\n",
    "        word.pop();\n",
    "        word.push('i');\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_2(word: &mut Vec<char>) {\n",
    "    let suffixes = [\n",
    "        (\"ational\", \"ate\"), (\"tional\", \"tion\"), (\"enci\", \"ence\"),\n",
    "        (\"anci\", \"ance\"), (\"izer\", \"ize\"), (\"abli\", \"able\"),\n",
    "        (\"alli\", \"al\"), (\"entli\", \"ent\"), (\"eli\", \"e\"),\n",
    "        (\"ousli\", \"ous\"), (\"ization\", \"ize\"), (\"ation\", \"ate\"),\n",
    "        (\"ator\", \"ate\"), (\"alism\", \"al\"), (\"iveness\", \"ive\"),\n",
    "        (\"fulness\", \"ful\"), (\"ousness\", \"ous\"), (\"aliti\", \"al\"),\n",
    "        (\"iviti\", \"ive\"), (\"biliti\", \"ble\")\n",
    "    ];\n",
    "\n",
    "    for &(suffix, replacement) in &suffixes {\n",
    "        if replace_suffix_condition(word, suffix, replacement, |stem| measure(stem) > 0) {\n",
    "            return;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_3(word: &mut Vec<char>) {\n",
    "    let suffixes = [\n",
    "        (\"icate\", \"ic\"), (\"ative\", \"\"), (\"alize\", \"al\"),\n",
    "        (\"iciti\", \"ic\"), (\"ical\", \"ic\"), (\"ful\", \"\"),\n",
    "        (\"ness\", \"\")\n",
    "    ];\n",
    "\n",
    "    for &(suffix, replacement) in &suffixes {\n",
    "        if replace_suffix_condition(word, suffix, replacement, |stem| measure(stem) > 0) {\n",
    "            return;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_4(word: &mut Vec<char>) {\n",
    "    let suffixes = [\n",
    "        \"al\", \"ance\", \"ence\", \"er\", \"ic\", \"able\", \"ible\",\n",
    "        \"ant\", \"ement\", \"ment\", \"ent\", \"ou\", \"ism\", \"ate\",\n",
    "        \"iti\", \"ous\", \"ive\", \"ize\"\n",
    "    ];\n",
    "\n",
    "    for &suffix in &suffixes {\n",
    "        if replace_suffix_condition(word, suffix, \"\", |stem| measure(stem) > 1) {\n",
    "            return;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if replace_suffix_condition(word, \"ion\", \"\", |stem| {\n",
    "        stem.ends_with(&['s']) || stem.ends_with(&['t']) && measure(stem) > 1\n",
    "    }) {}\n",
    "}\n",
    "\n",
    "fn step_5a(word: &mut Vec<char>) {\n",
    "    if word.ends_with(&['e']) {\n",
    "        let stem = &word[..word.len() - 1];\n",
    "        let m = measure(stem);\n",
    "        if m > 1 || (m == 1 && !ends_with_cvc(stem)) {\n",
    "            word.pop();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fn step_5b(word: &mut Vec<char>) {\n",
    "    if measure(word) > 1 && word.ends_with(&['l']) && word[word.len() - 2] == 'l' {\n",
    "        word.pop();\n",
    "    }\n",
    "}\n",
    "\n",
    "pub fn porter_stem(word: &str) -> String {\n",
    "    let mut word = word.to_lowercase().chars().collect::<Vec<_>>();\n",
    "\n",
    "    if word.len() <= 2 {\n",
    "        return word.into_iter().collect();\n",
    "    }\n",
    "\n",
    "    step_1a(&mut word);\n",
    "    step_1b(&mut word);\n",
    "    step_1c(&mut word);\n",
    "    step_2(&mut word);\n",
    "    step_3(&mut word);\n",
    "    step_4(&mut word);\n",
    "    step_5a(&mut word);\n",
    "    step_5b(&mut word);\n",
    "\n",
    "    word.into_iter().collect()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71e0d4",
   "metadata": {},
   "source": [
    "Następnie zaimplementowany parser, który pobiera dane z bazy SQL i przerabia je na strukture Document"
   ]
  },
  {
   "cell_type": "raw",
   "id": "401549ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "use rusqlite::{Connection, Result};\n",
    "use std::path::Path;\n",
    "\n",
    "#[derive(Debug)]\n",
    "pub struct Document {\n",
    "    pub id: i64,\n",
    "    pub title: String,\n",
    "    pub url: String,\n",
    "    pub text: String,\n",
    "}\n",
    "\n",
    "pub fn parse_sqlite_documents(db_path: &str) -> Result<Vec<Document>, rusqlite::Error> {\n",
    "    let conn = Connection::open(Path::new(db_path))?;\n",
    "\n",
    "    let mut stmt = conn.prepare(\"SELECT id, title, url, text FROM articles\")?;\n",
    "    let document_iter = stmt.query_map([], |row| {\n",
    "        Ok(Document {\n",
    "            id: row.get(0)?,       // i64\n",
    "            title: row.get(1)?,    // String\n",
    "            url: row.get(2)?,      // String\n",
    "            text: row.get(3)?,     // String\n",
    "        })// Dopasowanie do outputow z bazy ^\n",
    "    })?;\n",
    "\n",
    "    let mut documents = Vec::new();\n",
    "    for doc in document_iter {\n",
    "        documents.push(doc?);\n",
    "    }\n",
    "\n",
    "    Ok(documents)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0440ad0",
   "metadata": {},
   "source": [
    "Pozniej zakodowano tokenizer, ktorzy stowrzy alfabet uwzgledniajaca stop_words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "334ef8e0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "use std::collections::{HashSet, HashMap};\n",
    "use std::fs;\n",
    "use std::error::Error;\n",
    "use regex::Regex;\n",
    "use crate::document::parser::Document;\n",
    "use crate::stemer;\n",
    "\n",
    "pub fn load_stop_words(path: &str) -> Result<HashSet<String>, Box<dyn Error>> {\n",
    "    let content = fs::read_to_string(path)?;\n",
    "    let stop_words = content\n",
    "        .lines()\n",
    "        .map(|line| line.trim().to_lowercase())\n",
    "        .filter(|line| !line.is_empty())\n",
    "        .collect();\n",
    "    Ok(stop_words)\n",
    "}\n",
    "\n",
    "pub fn build_vocabulary(documents: &Vec<Document>, stop_words: &HashSet<String>) -> HashMap<String, usize> {\n",
    "    let re = Regex::new(r\"[a-zA-Z]+\").unwrap();\n",
    "    let mut terms = HashSet::new();\n",
    "\n",
    "    for doc in documents {\n",
    "        let full_text = format!(\"{} {}\", doc.title, doc.text);\n",
    "        for word in re.find_iter(&full_text.to_lowercase()) {\n",
    "            let term = word.as_str().to_string();\n",
    "            if !stop_words.contains(&term) {\n",
    "                let stemmed = stemer::porter_algorithm::porter_stem(&term);\n",
    "                terms.insert(stemmed);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    let mut term_list: Vec<String> = terms.into_iter().collect();\n",
    "    term_list.sort();\n",
    "    term_list\n",
    "        .into_iter()\n",
    "        .enumerate()\n",
    "        .map(|(i, term)| (term, i))\n",
    "        .collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef672373",
   "metadata": {},
   "source": [
    "Budowanie macierzy i IDF (bez SVD)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c1a291",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "use std::collections::{HashMap, HashSet};\n",
    "use sprs::{CsMat, TriMat};\n",
    "use crate::document::parser::Document;\n",
    "pub struct TfIdfMatrix {\n",
    "    pub terms: HashMap<String, usize>,\n",
    "    pub matrix: CsMat<f64>,\n",
    "    pub idf: Vec<f64>,\n",
    "}\n",
    "\n",
    "impl TfIdfMatrix {\n",
    "    pub fn build(documents: &[Document], terms: &HashMap<String, usize>) -> Self {\n",
    "        let n_docs = documents.len();\n",
    "        let n_terms = terms.len();\n",
    "\n",
    "        let mut df = vec![0; n_terms];\n",
    "        let mut triplets = Vec::new();\n",
    "\n",
    "        for (doc_index, doc) in documents.iter().enumerate() {\n",
    "            let mut term_counts = HashMap::new();\n",
    "            let mut seen_terms = HashSet::new();\n",
    "\n",
    "            let text = format!(\"{} {}\", doc.title, doc.text);\n",
    "            let lowercased = text.to_lowercase();\n",
    "            let tokens = lowercased\n",
    "                .split_whitespace()\n",
    "                .map(|t| t.trim_matches(|c: char| !c.is_alphabetic()));\n",
    "\n",
    "            let mut total_terms = 0;\n",
    "\n",
    "            for token in tokens {\n",
    "                if let Some(&term_index) = terms.get(token) {\n",
    "                    *term_counts.entry(term_index).or_insert(0) += 1;\n",
    "                    if seen_terms.insert(term_index) {\n",
    "                        df[term_index] += 1;\n",
    "                    }\n",
    "                    total_terms += 1;\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for (term_index, count) in term_counts {\n",
    "                let tf = count as f64 / total_terms as f64;\n",
    "                triplets.push((term_index, doc_index, tf));\n",
    "            }\n",
    "        }\n",
    "\n",
    "        let idf: Vec<f64> = df\n",
    "            .iter()\n",
    "            .map(|&df| {\n",
    "                if df == 0 {\n",
    "                    0.0\n",
    "                } else {\n",
    "                    (n_docs as f64 / df as f64).ln()\n",
    "                }\n",
    "            })\n",
    "            .collect();\n",
    "\n",
    "        let tf_idf_triplets: Vec<_> = triplets\n",
    "            .into_iter()\n",
    "            .map(|(term_index, doc_index, tf)| {\n",
    "                let value = tf * idf[term_index];\n",
    "                (term_index, doc_index, value)\n",
    "            })\n",
    "            .collect();\n",
    "\n",
    "        let mut tri_mat = TriMat::new((n_terms, n_docs));\n",
    "        for (row, col, val) in tf_idf_triplets {\n",
    "            tri_mat.add_triplet(row, col, val);\n",
    "        }\n",
    "\n",
    "        let mut matrix = tri_mat.to_csc();\n",
    "\n",
    "        for mut col in matrix.outer_iterator_mut() {\n",
    "            let norm = col.iter().map(|(_, v)| v * v).sum::<f64>().sqrt();\n",
    "            if norm > 0.0 {\n",
    "                for (_, value) in col.iter_mut() {\n",
    "                    *value /= norm;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        Self {\n",
    "            terms: terms.clone(),\n",
    "            matrix,\n",
    "            idf,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5656b9c0",
   "metadata": {},
   "source": [
    "Znajdowanie cosinusowego podobienstwa"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8ca23f4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "use std::collections::HashMap;\n",
    "use sprs::CsVec;\n",
    "use crate::matrix::TfIdfMatrix;\n",
    "\n",
    "pub fn search(query: &str, tfidf: &TfIdfMatrix, k: usize) -> Vec<(usize, f64)> {\n",
    "    let mut query_tf: HashMap<usize, usize> = HashMap::new();\n",
    "    let mut total_terms = 0;\n",
    "\n",
    "    let binding = query\n",
    "        .to_lowercase();\n",
    "    let q_tokens = binding\n",
    "        .split_whitespace()\n",
    "        .map(|t| t.trim_matches(|c: char| !c.is_alphabetic()));\n",
    "\n",
    "    for token in q_tokens {\n",
    "        if let Some(&idx) = tfidf.terms.get(token) {\n",
    "            *query_tf.entry(idx).or_insert(0) += 1;\n",
    "            total_terms += 1;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    let mut indices = Vec::new();\n",
    "    let mut data = Vec::new();\n",
    "    for (term_idx, count) in query_tf {\n",
    "        let tf = count as f64 / total_terms as f64;\n",
    "        let idf = tfidf.idf[term_idx];\n",
    "        indices.push(term_idx);\n",
    "        data.push(tf * idf);\n",
    "    }\n",
    "    let query_vec = CsVec::new(tfidf.terms.len(), indices, data);\n",
    "    let norm_query = query_vec.iter().map(|(_, v)| v * v).sum::<f64>().sqrt();\n",
    "    let normalized_q = if norm_query > 0.0 {\n",
    "        query_vec.map(|v| v / norm_query)\n",
    "    } else {\n",
    "        query_vec.clone()\n",
    "    };\n",
    "    \n",
    "    let mut similarities = Vec::new();\n",
    "    for doc_idx in 0..tfidf.matrix.cols() {\n",
    "        let doc_vec = tfidf.matrix.outer_view(doc_idx).unwrap();\n",
    "        let sim = normalized_q.dot(&doc_vec).abs();  // |cos θ|\n",
    "        similarities.push((doc_idx, sim));\n",
    "    }\n",
    "    \n",
    "    similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n",
    "    similarities.truncate(k);\n",
    "    similarities\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd67f3",
   "metadata": {},
   "source": [
    "### Analiza podejscia pierwszego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7bbe4",
   "metadata": {},
   "source": [
    "Build cargo: 2min\n",
    "\n",
    "Czas kompilacji Rust: 10s\n",
    "\n",
    "Czas obliczania macierzy: 10-20 min ( tak dlugi czas spowodowany glownie stemmingiem)\n",
    "\n",
    "#### Wydajnosc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae8680",
   "metadata": {},
   "source": [
    "Do przetestowawania wydajnosci uzylem fraz np:\n",
    "Cracow, War, Programming Languages, new Pope, Fruit and Vegetables, Best Country In Europe, itd..\n",
    "\n",
    "Wyniki będą zależne od czasu wyszukiwania , wyniku najlepszego dopasowania i dlugości frazy wprowadzonej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c86841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# czas wyszukiwania\n",
    "T = [0.36,0.7,0.7,0.69,0.2,0.66,0.8,0.73,0.33,0.4]\n",
    "# najlepszy score\n",
    "S = [1,1,1,0.91,0,0.7,0.5,0.27,0.6,0.67]\n",
    "# długość wpisywanie frazy\n",
    "L = [1,1,3,2,1,4,3,3,2,2]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
